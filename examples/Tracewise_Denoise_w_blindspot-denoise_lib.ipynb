{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELF-SUPERVISED DENOISING: PART THREE (Using Entrypoints)\n",
    "### Modernized Implementation\n",
    "\n",
    "This notebook demonstrates how to use the entrypoint scripts to reproduce the functionality of Tutorial 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook uses the `train` and `infer` entrypoint scripts to:\n",
    "1. Generate test data\n",
    "2. Train a blind-trace denoising model\n",
    "3. Run inference and visualize results\n",
    "\n",
    "The entrypoint scripts provide a clean interface to the training and inference functionality, making it easy to reproduce results and experiment with different parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Import our module functions for visualization and data handling\n",
    "from blindspot_denoise.models import UNet\n",
    "from blindspot_denoise.utils import add_trace_wise_noise, set_seed\n",
    "from blindspot_denoise.preprocessing import multi_active_pixels\n",
    "\n",
    "# Set plotting parameters\n",
    "%matplotlib inline\n",
    "cmap = 'seismic'\n",
    "vmin = -0.5\n",
    "vmax = 0.5\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Test Data\n",
    "\n",
    "First, we'll generate a test dataset of random seismic-like events. Alternatively, you can use your own data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "test_data_path = \"tests/test_data.npy\"\n",
    "print(\"Generating test dataset...\")\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"tests/generate_test_data.py\",\n",
    "        \"--output\", test_data_path,\n",
    "        \"--n-samples\", \"50\",\n",
    "        \"--n-traces\", \"64\",\n",
    "        \"--n-time-samples\", \"128\",\n",
    "        \"--seed\", \"42\"\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\", result.stderr)\n",
    "\n",
    "# Load and visualize the generated data\n",
    "data = np.load(test_data_path)\n",
    "print(f\"\\nData shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from the generated data\n",
    "plt.figure(figsize=[7, 5])\n",
    "plt.imshow(data[0], cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "plt.title('Sample from Generated Test Data')\n",
    "plt.xlabel('Trace')\n",
    "plt.ylabel('Time Sample')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Trace-wise Noise\n",
    "\n",
    "Let's add trace-wise noise to the data to simulate the noisy input we want to denoise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add trace-wise noise to the data\n",
    "print(\"Adding trace-wise noise...\")\n",
    "noisy_patches = add_trace_wise_noise(\n",
    "    data,\n",
    "    num_noisy_traces=5,\n",
    "    noisy_trace_value=0.0,\n",
    "    num_realisations=7,\n",
    ")\n",
    "\n",
    "# Randomize patch order\n",
    "shuffler = np.random.permutation(len(noisy_patches))\n",
    "noisy_patches = noisy_patches[shuffler]\n",
    "\n",
    "print(f\"Noisy patches shape: {noisy_patches.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some noisy patches\n",
    "fig, axs = plt.subplots(3, 6, figsize=[25, 17])\n",
    "for i in range(6 * 3):\n",
    "    axs.ravel()[i].imshow(noisy_patches[i], cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "    axs.ravel()[i].set_title(f'Patch {i}')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Preprocessing (Active Pixel Corruption)\n",
    "\n",
    "Let's visualize what happens during the preprocessing step where active pixels are corrupted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corruption process\n",
    "crpt_patch, mask = multi_active_pixels(\n",
    "    noisy_patches[0],\n",
    "    active_number=3,\n",
    "    noise_level=0.5\n",
    ")\n",
    "\n",
    "# Use the pre-made plotting function to visualise the corruption\n",
    "fig, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "axs[0].imshow(noisy_patches[0], cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "axs[1].imshow(crpt_patch, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "axs[2].imshow(mask, cmap='binary_r', aspect='auto')\n",
    "\n",
    "axs[0].set_title('Original')\n",
    "axs[1].set_title('Corrupted')\n",
    "axs[2].set_title('Corruption Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model\n",
    "\n",
    "Now we'll use the `train` entrypoint to train the denoising model. This will save checkpoints to the output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the noisy data for training\n",
    "# Save the noisy patches to a file that the training script can use\n",
    "training_data_path = \"tests/training_data.npy\"\n",
    "np.save(training_data_path, data)\n",
    "\n",
    "# Set up training parameters\n",
    "output_dir = \"./checkpoints\"\n",
    "n_epochs = 20  # Reduced for notebook demo; use 150-200 for best results\n",
    "n_training = 2048\n",
    "n_test = 256\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {n_epochs} epochs\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# Run training using the entrypoint\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        sys.executable, \"-m\", \"blindspot_denoise.train\",\n",
    "        \"--data\", training_data_path,\n",
    "        \"--output-dir\", output_dir,\n",
    "        \"--n-epochs\", str(n_epochs),\n",
    "        \"--n-training\", str(n_training),\n",
    "        \"--n-test\", str(n_test),\n",
    "        \"--batch-size\", str(batch_size),\n",
    "        \"--hidden-channels\", \"32\",\n",
    "        \"--levels\", \"2\",\n",
    "        \"--num-noisy-traces\", \"5\",\n",
    "        \"--num-realisations\", \"7\",\n",
    "        \"--active-number\", \"15\",\n",
    "        \"--noise-level\", \"0.25\",\n",
    "        \"--seed\", \"42\",\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Metrics\n",
    "\n",
    "Load and plot the training history to see how the model learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "history_path = Path(output_dir) / \"training_history.npz\"\n",
    "if history_path.exists():\n",
    "    history = np.load(history_path)\n",
    "    train_loss_history = history['train_loss']\n",
    "    train_accuracy_history = history['train_accuracy']\n",
    "    test_loss_history = history['test_loss']\n",
    "    test_accuracy_history = history['test_accuracy']\n",
    "    \n",
    "    # Plot training metrics\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    \n",
    "    axs[0].plot(train_accuracy_history, 'r', lw=2, label='train')\n",
    "    axs[0].plot(test_accuracy_history, 'k', lw=2, label='validation')\n",
    "    axs[0].set_title('RMSE', size=16)\n",
    "    axs[0].set_ylabel('RMSE', size=12)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel('# Epochs', size=12)\n",
    "    \n",
    "    axs[1].plot(train_loss_history, 'r', lw=2, label='train')\n",
    "    axs[1].plot(test_loss_history, 'k', lw=2, label='validation')\n",
    "    axs[1].set_title('Loss', size=16)\n",
    "    axs[1].set_ylabel('Loss', size=12)\n",
    "    axs[1].legend()\n",
    "    axs[1].set_xlabel('# Epochs', size=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training history not found. Training may still be running or failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Inference\n",
    "\n",
    "Now we'll use the `infer` entrypoint to denoise a noisy sample. We'll create a new noisy realization and denoise it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new noisy realization for testing (different from training)\n",
    "test_sample_idx = 0\n",
    "testdata = add_trace_wise_noise(\n",
    "    data[test_sample_idx:test_sample_idx+1],\n",
    "    num_noisy_traces=5,\n",
    "    noisy_trace_value=0.0,\n",
    "    num_realisations=1\n",
    ")[0]\n",
    "\n",
    "print(f\"Test data shape: {testdata.shape}\")\n",
    "\n",
    "# Save test data\n",
    "test_input_path = \"tests/test_input.npy\"\n",
    "np.save(test_input_path, testdata)\n",
    "\n",
    "# Visualize the noisy test data\n",
    "plt.figure(figsize=[7, 5])\n",
    "plt.imshow(testdata, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "plt.title('Noisy Test Data')\n",
    "plt.xlabel('Trace')\n",
    "plt.ylabel('Time Sample')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model checkpoint (use final or last epoch)\n",
    "model_path = Path(output_dir) / \"denoise_final.net\"\n",
    "if not model_path.exists():\n",
    "    # Try to find the last epoch checkpoint\n",
    "    checkpoints = sorted(Path(output_dir).glob(\"denoise_ep*.net\"))\n",
    "    if checkpoints:\n",
    "        model_path = checkpoints[-1]\n",
    "        print(f\"Using checkpoint: {model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model checkpoint found in {output_dir}\")\n",
    "\n",
    "# Run inference\n",
    "output_path = \"tests/denoised_output.npy\"\n",
    "print(f\"Running inference with model: {model_path}\")\n",
    "print(f\"Input: {test_input_path}\")\n",
    "print(f\"Output: {output_path}\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        sys.executable, \"-m\", \"blindspot_denoise.infer\",\n",
    "        \"--model\", str(model_path),\n",
    "        \"--input\", test_input_path,\n",
    "        \"--output\", output_path,\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results\n",
    "\n",
    "Compare the original clean data, noisy data, denoised result, and the removed noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "clean_data = data[test_sample_idx]\n",
    "denoised_data = np.load(output_path)\n",
    "\n",
    "# Visualize denoising performance\n",
    "fig, axs = plt.subplots(1, 4, figsize=[20, 5])\n",
    "axs[0].imshow(clean_data, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "axs[1].imshow(testdata, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "axs[2].imshow(denoised_data, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "axs[3].imshow(testdata - denoised_data, aspect='auto', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "axs[0].set_title('Clean (Original)')\n",
    "axs[1].set_title('Noisy')\n",
    "axs[2].set_title('Denoised')\n",
    "axs[3].set_title('Noise Removed')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Trace')\n",
    "    ax.set_ylabel('Time Sample')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate some quantitative metrics\n",
    "mse_noisy = np.mean((clean_data - testdata)**2)\n",
    "mse_denoised = np.mean((clean_data - denoised_data)**2)\n",
    "psnr_noisy = -10 * np.log10(mse_noisy / (np.max(clean_data) - np.min(clean_data))**2)\n",
    "psnr_denoised = -10 * np.log10(mse_denoised / (np.max(clean_data) - np.min(clean_data))**2)\n",
    "\n",
    "print(\"Quantitative Metrics:\")\n",
    "print(f\"  MSE (Noisy): {mse_noisy:.6f}\")\n",
    "print(f\"  MSE (Denoised): {mse_denoised:.6f}\")\n",
    "print(f\"  PSNR (Noisy): {psnr_noisy:.2f} dB\")\n",
    "print(f\"  PSNR (Denoised): {psnr_denoised:.2f} dB\")\n",
    "print(f\"  Improvement: {psnr_denoised - psnr_noisy:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Direct Python API Usage\n",
    "\n",
    "Instead of using the entrypoint scripts, you can also use the module functions directly in Python. This gives you more control over the training loop and allows for custom callbacks or modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using the module directly for custom inference\n",
    "from blindspot_denoise.models import UNet\n",
    "from blindspot_denoise.utils import set_seed\n",
    "\n",
    "# Load model\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "network = torch.load(model_path, map_location=device)\n",
    "network.eval()\n",
    "\n",
    "# Run inference directly\n",
    "torch_testdata = torch.from_numpy(\n",
    "    np.expand_dims(np.expand_dims(testdata, axis=0), axis=0)\n",
    ").float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_prediction = network(torch_testdata.to(device))\n",
    "    test_pred = test_prediction.detach().cpu().numpy().squeeze()\n",
    "\n",
    "print(f\"Direct inference result shape: {test_pred.shape}\")\n",
    "print(\"This matches the entrypoint output!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Data Generation**: Using the test data generator to create synthetic seismic-like data\n",
    "2. **Training**: Using the `train` entrypoint to train a blind-trace denoising model\n",
    "3. **Inference**: Using the `infer` entrypoint to denoise noisy seismic data\n",
    "4. **Visualization**: Comparing clean, noisy, and denoised results\n",
    "\n",
    "The entrypoint scripts provide a clean, reproducible way to train and use the models, while the module functions can be imported directly for custom workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
